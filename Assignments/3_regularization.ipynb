{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Some personnal imports\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10 # 0, 1, ..., 9, image of number\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "\n",
    "  # float 1-hot encodings: Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n",
    "\n",
    "  return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    \n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both **logistic** and **neural network** models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "- Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # 1 Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "    \n",
    "  # 2 Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    \n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    \n",
    "  # 3 Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    \n",
    "  loss = tf.reduce_mean( # L2 penalty (Note: should specify arguments in softmax_cross_entropy_with_logits)\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels)) \\\n",
    "                                            + beta_regul * tf.nn.l2_loss(weights)\n",
    "  \n",
    "    \n",
    "  # 4 Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "\n",
    "  # 5 Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-dd53f57ad158>:4: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 23.509336\n",
      "Minibatch accuracy: 3.1%\n",
      "Validation accuracy: 10.5%\n",
      "Minibatch loss at step 500: 2.951400\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 1000: 1.703904\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 1500: 1.052881\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 2000: 1.208335\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 2500: 0.527465\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 3000: 0.826048\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.0%\n",
      "Test accuracy: 88.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "    \n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    \n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict = feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L2 regularization introduces a new meta parameter that should be tuned. Since I do not have any idea of what should be the right value for this meta parameter, I will plot the accuracy by the meta parameter value (in a logarithmic scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-d31fc0f30837>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-d31fc0f30837>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-d31fc0f30837>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-d31fc0f30837>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-d31fc0f30837>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-d31fc0f30837>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-d31fc0f30837>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-d31fc0f30837>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-d31fc0f30837>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-d31fc0f30837>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-d31fc0f30837>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-d31fc0f30837>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-d31fc0f30837>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-d31fc0f30837>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-d31fc0f30837>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-d31fc0f30837>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-d31fc0f30837>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-d31fc0f30837>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-d31fc0f30837>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-d31fc0f30837>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "    # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEMCAYAAADEXsFmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14743a150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1-layer neural network using same technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # 1 Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "    \n",
    "  # 2 Variables.\n",
    "  weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "\n",
    "  weights2 = tf.Variable(tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    \n",
    "  # 3 Training computation (L2 norm)\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "  \n",
    "\n",
    "  # 4 Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "    \n",
    "  # 5 Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the 1-layer neural network model and print the results. It may take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-60972b56e30f>:4: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 631.116211\n",
      "Minibatch accuracy: 20.3%\n",
      "Validation accuracy: 31.3%\n",
      "Minibatch loss at step 500: 193.898865\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 1000: 115.514580\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 1500: 68.680923\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 2000: 41.756123\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 2500: 25.030838\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 3000: 15.545663\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.5%\n",
      "Test accuracy: 93.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "    \n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    \n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict = feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally something above 90%! I will also plot the final accuracy by the L2 parameter to find the best value. It may take much more time than before. (电脑会呼呼作响)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-d1c322e5d263>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-d1c322e5d263>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-d1c322e5d263>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-d1c322e5d263>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-d1c322e5d263>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-d1c322e5d263>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-d1c322e5d263>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-d1c322e5d263>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-d1c322e5d263>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-d1c322e5d263>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-d1c322e5d263>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-d1c322e5d263>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-d1c322e5d263>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-d1c322e5d263>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-d1c322e5d263>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-d1c322e5d263>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-d1c322e5d263>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-d1c322e5d263>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-d1c322e5d263>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-d1c322e5d263>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:    \n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    \n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEMCAYAAADUEk3/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd8FHX+x/HXJ50khBBCCgkhQGhSQ0JTUBT1UBEFe/cs\nnJ5nPe93np6e53nqFdud56lnRREVC4K9xkqXFnpNIQQCpJDevr8/dsAYUzbJbia7+3k+HnkkO20/\nM/vNe2dnvjsjxhiUUkp5Pj+7C1BKKeUaGuhKKeUlNNCVUspLaKArpZSX0EBXSikvoYGulFJeQgNd\nuZSIhIiIEZFEu2tpKxFZKiKXdmD+HSIyycU1BYtIqYj0ceVyGyz/URG5rp3zTheR7a6uyW4iMk5E\nMuyuoz18LtCtf44jP/UiUtHg8SUdWG6HwkB5PmPMQGPMko4so3E7MsZUGWPCjTF5Ha/wZ8+VAJwL\nPG89DhORt0Uky3pTnujq5+xqmtoBMcasAOpF5BQbS2sXnwt0658j3BgTDmQDZzYYNs/u+txFRALs\nrqGjuuo6dNW6nHAVsNAYU209NkAGcBFQaFdRLenEbT0P+FUnPZfrGGN89gfYDZzcaJg/cDewEziA\n44WNtMaFAa8Bh4AiYBnQE3gYqAMqgVLg4SaeKwB4C9hnzfslMKTB+DDgX0AOUAx8BQRY46YCS63h\n2cDF1vClwKUNlnEd8Jn1dwiOf9DrgR3AZmv4f4FcoARYDkxsVOOfrHUvAVYAccBzwF8brc8nwPVN\nrOeR5/2NtX0LgL8CAoRayx3UYPpEoPzINm60rOuAL4D/4AiYP1rDfwVssV6H94GEBvOcAWyztvFj\nDbcR8BDwbINphwK1DR43nHYojnA7ZK3DS0D3BtPmA7cDG4DyBsMm42hDpQ1+yqxtEgf0Bj60lnkI\neBeIt+b/WTtqsD0TrWmigFet+XcB/wdIg+31OY52VGS97ic33q4N1uF74Nxmxh1o2DaamWY6sL3B\n43usmg4DmcAZ1vBWX3dgFrDOqvsb4JiWtnUzbW6Otc6FwKONpmmyzeD4HzDWa1QKnG0NH2ith7/d\nOdWmTLO7AFtXvulA/73VoPpYDeVF4AVr3M3Am0A3HOE3Dgizxv0kXJt4rgDgciDcWu5/gaUNxj+H\nIyTjrECYYv1OsRraOdYyegOjm3pOmg7094FIoJs1/HIcb0KBwF043kACrXF3A6ut5/QDUq15j7f+\nUY8ERx/rnzGqifU88rwfW/P2x/EGcSQonwf+3Gh7L2hmm10H1ALXWtuiG3ABsAkYbK3D/cCX1vTx\n1raaYY37P6CG9gf6SUCQ9ZosBR5qMG0+jje8Pg22bT4wuYn1eAT4zFqHWOAsa1164Aj015qqodH2\nPBLobwALrHaUYr0ulzTYXjXWa+wP3ArsbqFNHgZGNjOuPYF+gfUa+AGXWcuPbu11ByYCe4E0q+45\nwFZ+3KH52bZups29DURYba4ImNqgrubazE+2b6PlVgOD7c6ptvzYXoCtK990oO8CjmvwuD+O8BLg\n1zj2nEc0sawWA72J6eOAeqtBBVr/iEOamO7PwPxmluFMoB/bQg1irdsQ63EW8ItmptsJTLEe3w68\n3cwyjzzv1AbDbgPet/4+oVEIrAdmNrOs64CtjYZ9iRVg1uMj2y7WCoIvG4zzA/bTjkBvopYLgSUN\nHudjfVJqNGxyo2GXA9tp4s3PGj8R2NvCa3o0cIBgHHvwAxqMvxn4qMH2ymwwLsqat6lPP/7WuORm\n6mpzoDcxfvOR9tTS6w68ANzVaN4sYEJz27qZNpfeYNgi4BYn2kxLgX4QGN/SNuhqPz53DL0lIiJA\nX+ADESkSkSIce6x+QC8ce9FfAW+KSK6IPCAi/k4uO0BEHhaRnSJSgqOxi7XceBx73zuamLVvM8Od\nldOojj+IyBYRKcbx0TQEiLbWPaGp5zKO1j0XOHKy7lLg5TY8bxaOvSuArwF/EZkkImNwrPuHztYP\n9AOeavD6FODYi0+0nuPo9MaYemBPK3U2SUT6iMgCEdljvV7PAtGt1NZ4GRNwHDY5yxhzyBrWXUSe\nF5Fsa7mfNLHc5sThaIvZDYZl4Xjdjshv8He59Tu88YKMMXU49qC7O/PEIjK4QeeBA81Mc7WIrGvw\n2qTw47q19Lr3A+48Mp81b+9G69XitrY0Xvcj691Sm2lJdxx7+h5DA70BK7j2ACcZYyIb/IQYYw4Y\nR4+De4wxQ3EchjgPx54bON7lW/JL4BTgRBwftYdawwXHx81aHMftGstpZjg4jvuFNngc19RqHfnD\nOmt/I47jlZE49uAqcBxKObLuzT3XXOBcEUnD8SbzfjPTHdG3wd9JQB787M3hMhyHG2paWE7j7ZoD\nXNno9elmjFmFYzse/ScVET9+GgrObK8j/mFNP8IYEwFcg+O1aqm2o6xuhm8B1xhjNjQYdYdV4zhr\nuac2Wm5L7Sgfx6e6pAbDkmjnmxaOY9aDnZnQGLPV/Nh54GdvQCIyGPg3jk9JUcaYSByfTMSav6XX\nPQe4p9FrGmqMebthCe1cxyPLb67NNLlcERkIVNGxnalOp4H+c08BD4lIXwARiRGRM62/TxaRY6yg\nKMERwvXWfPuAAS0stzuOk10HcZwAvf/ICKthzwUeF5FYEfEXkcnW3v/LwAwRmWXt5fcWkVHWrGtw\nhGyIiAwFrmxl3brj+KhZgOPY8H049tCPeBZ4QEQGiEOqiERaNe4ENuL4ePy6+bFnRHN+LyI9RCQZ\nxwnS1xuMmwucj6M3xdxWltPYU8AfRWQIgIj0FJFzrHGLgAkicrrVG+I2HOcLjlgDnCgiCSLSE8dx\n3OZ0x3E8vkREkqxlOUVEgnAcz33aGPNuE8stB4pEJBr4Y6PxzbYjY0wV8A6O1yjMCp2bgVecra2R\nD3AcCmlYe7CIHGkTQQ3+bk04jv+FAsDP6tue0mia5l73Z4AbRSTdanfhIjJTREJxjWbbjLVNi/n5\nNj8B+NT6JOMxNNB/7u84TmB9ISKHcfQEGGuNS8BxEuvIWfwP+DGoHgUuF5FCEfl7E8t9Dkdjz8dx\n/PDbRuNvwrE3sBpH6P8Fx57zdhwn0e7EcYZ+JTC8Qa0B1nKfofV/7MU4Pvru4MdePAUNxj+EY8/7\nCxxvWE/hOG57xEvASFo/3IK1nLVWvQsa1maM2YGjx8FhY8xyJ5Z1lDFmPvAE8LZ1yGINjk8+GGP2\n4giLf1nrlohjW1c1qOk9HG9MS4GFLTzVPTh6rBTjCNG32lDmAGACjje1ht97iAH+ieMwxEEcbeCD\nRvO21o6OdKXLwvE6PYujJ1Z7vAicbb0BHZGF41NbLxyHFytEpKVPMgAYY37A0V5W4vik1N/6u+E0\nTb7uxpjvcLT/p3Ec4tgKXEzH9sobPm+zbcZyD7DAOiQz0xp2ibU+HuVIrwWlWiUipwJPGmMa73m1\nZ1mvAhuNMfe3OnH7nyMAxxvomaaDX/jxViLyCI4Tz50SXp3xuneUiIwD/mmMOaHVibsYDXTllAaH\nEb42xjS159iWZaUAPwDDjDHtPf7b3LJPw/GpqgpHt8wrgBQnDhEpN3Pn664c9JCLapXVK6EQx/Hf\n/3RwWX/HcVjpPjf9Ux/pM78fmAbM0jC3Xye87grdQ1dKKa+he+hKKeUlNNCVUspLdOpV4qKjo01y\ncnK75i0rKyMsLMy1BSnlJG1/yk6rVq06YIzp3dp0nRroycnJrFy5svUJm5CRkcHUqVNdW5BSTtL2\np+wkIlnOTKeHXJRSyktooCullJfQQFdKKS+hga6UUl5CA10ppbyEBrpSSnkJDXSlurC6esP2/aWs\nyjpkdynKA3RqP3SlVPNKKmvYvPcwm/aWHP3Zsu8wlTWOe6g8dsEYzk5NaGUpypdpoCvVyYwx5Byq\nYGOD4N6UX0LOoYqj00SGBjIsLoKLx/fjmD4RzF+ezd0LM0lP7kliT1fdyEd5Gw10pdrAGENtvaG6\ntt7xU9fod209NdbfVXU/Pi6uqGFL/mE25pWwOf8wpVW1AIhA/15hjEqM5MJxSQyL786w+AjiIkJw\n3LfbYUL/KE57/Btue2Mt86+diL9f49ubKqWBrlSrFq3N464vyqj67ENq6upp7xWnw4MDGBrXnVmp\nCQyLj2BYfHeGxHUnNKj1f8O+UaHcO3M4ty9YyzNf7+T6qc3dy1v5Mg10pVqwOruQ2xesJSHUj1NT\nkwn29yMowI9A63dQgB9B/j/93dS40CB/+vTohl8H9qzPGZvAF5v38cinW5gyKJoRCT1cuKbKGzgV\n6CJyM3AtIMD/jDGPichfcNy8uB7H3WGuNMbkua1SpTrZ3uIK5ry8iriIEH47BmacOszWekSEB2aN\nZFVWITe/tpr3bpxCtyB/W2tSXUur3RZFZASOMB8PjAZmWPcG/IcxZpQxZgyOO6nf49ZKlepEFdV1\nXDt3JRXVdTx7RTrhQV3jmHVkaBAPnzeGHQVlPPjhJrvLUV2MM/3QhwHLjDHlxpha4CtgtjGmpME0\nYYDey055BWMMt7+5lg15JfzrojEMju1ud0k/MXlQNFdP7s/cJVl8uXm/3eWoLqTVe4qKyDDgXWAS\nUAF8Dqw0xtwoIn8FLgeKgRONMQVNzD8HmAMQGxub9tprr7Wr0NLSUsLDw9s1r1Jt8e72at7ZXsP5\nQwI5vX8Q0PXaX3Wd4b4lFZRUw/2TuxHRRT5BKPc48cQTVxlj0lubzqmbRIvI1cCvgTJgA1BljLml\nwfg/ACHGmD+1tJz09HSjN7hQXdlHmXu57pUfmJ2awMPnjz7adbArtr9Ne0s464nvOGFIb565LO0n\n3RyVdxERpwLdqa/+G2OeM8akGWOOBwqBrY0mmQec0/Yyleo6NuQVc+vra0lNiuSB2SO7fEAOi4/g\n/6YP4dON+3h9RY7d5aguwKlAF5EY63cSMBt4VUQGNZjkLGCz68tTqnMUHK7i2pdWEhkayNOXpRES\n6Bm9R646rj/HpfTiz4s3sutAmd3lKJs5e3Gut0RkI7AYuMEYUwQ8JCKZIrIOOBW42V1FKuVOVbV1\nXPfKKg6VV/O/y9OJ6R5id0lO8/MT/nneaIIC/Ljl9TXU1NXbXZKykbOHXKYYY44xxow2xnxuDTvH\nGDPC6rp4pjFmj3tLVcr1jDH88Z1MVmUV8vB5YzzyyzrxPbrxwKyRrM0p4t9fbLe7HGUjvXyu8mnP\nfbuLBatyuWnaIM4YFW93Oe12xqh4Zo9N4IkvtrEqq9DucpRNNNCVz/pyy34e+GATp42I45Zpg1qf\noYv788zh9Insxq2vrzl68S/lWzTQlU/avv8wN726mqFxETx8/ugOXWOlq+geEsijF4wht7Cc+xZv\nsLscZQMNdOVzisqrueallQQH+vG/K9KdutqhpxiXHMWvp6bwxspcPsrca3c5qpNpoCufUlNXzw2v\n/kBeUSVPX5ZGQmQ3u0tyuZtPHsSoxB7c8fZ69pVU2l2O6kQa6Mqn3P/eRr7bfpC/zhpBWr8ou8tx\ni0B/Px69YAyVNXXcvmAt9fV6mSVfoYGufMa8ZVm8tCSLa6f057z0vnaX41YDe4fzxzOO4ZttB3hp\nyW67y1GdRANd+YQlOw7yp3c3MHVIb+44zd7rmneWSyYkMW1oDA9+uJmt+w7bXY7qBBroyuvlFpZz\n/bxVJEeH8a+LUn3mfpwiwt/OHUVESAA3v7aGiuo6u0tSbqaBrrze89/upryqjmcvTyciJNDucjpV\ndHgw/zh3NJvzS7j6pRUa6l5OA115tbp6w+J1eUwd0pvk6DC7y7HFiUNjeOT80SzZeVBD3ctpoCuv\ntmznQQoOV3HWmAS7S7HVrNREDXUfoIGuvNqitXmEBfkzbViM3aXYTkPd+2mgK69VVVvHh5n5nDo8\nzmOub+5uGureTQNdea2vtx6guKKGmaP72F1Kl6Kh7r000JXXWrQ2j56hgUweFG13KV2Ohrp30kBX\nXqm8upbPNu7j9JHxBPprM2+Khrr30ZauvNKnG/dRUVOnh1taoaHuXTTQlVdatCaP+B4hjEv2zgtw\nuZKGuvfQQFdep6i8mq+3FXDm6D5eceOKzqCh7h000JXX+TAzn5o6o4db2khD3fNpoCuv8+6aPQyI\nDmN4nwi7S/E4GuqeTQNdeZX84kqW7TrEmaP7IKKHW9pDQ91zaaArr/LeujyMgZlj9HBLR2ioeyYN\ndOVVFq/NY0RCBAN7h9tdisdrHOqVNRrqXZ0GuvIauw6UsTa3WE+GutCRUP9+x0F+9+Y6jNH7k3Zl\nAXYXoJSrLF6bB8CMURrorjQrNZH84ir+9tFmBvYO45aTB9tdkmqGBrryCsYYFq3NY3z/KPpEdrO7\nHK9z3QkD2FFQymOfbaN/dJjPX1++q9JDLsorbNp7mO37S/Vwi5uICA/MGsn4/lH87s11rMoqtLsk\n1QQNdOUV3l27hwA/4fSR8XaX4rWCAvx46tI04nuE8KuXV5JzqNzuklQjGujK49XXG95bu5cpg6KJ\nCguyuxyvFhUWxHNXjKO6tp5rXlrJ4coau0tSDWigK4/3Q3Yhe4oqtO95J0mJCee/l6axvaCUG+ev\nprau3u6SlEUDXXm8RWvzCA7w45Rj4uwuxWcclxLNX84aQcaWAu5/f5Pd5SiLU4EuIjeLSKaIbBCR\nW6xh/xCRzSKyTkTeEZFI95aq1M/V1tXz/rq9nDwslvBg7bTVmS6ekMTVk/vz4ve7eXnJbrvLUTgR\n6CIyArgWGA+MBmaISArwKTDCGDMK2Ar8wZ2FKtWU73Yc5GBZtR5uscmdpw/jpKEx3Lt4I19vLbC7\nHJ/nzB76MGCZMabcGFMLfAXMNsZ8Yj0GWAokuqtIpZqzaE0e3UMCmDqkt92l+CR/P+FfF6UyKCac\nG+b9wLZ9h+0uyac5E+iZwBQR6SUiocDpQN9G01wFfOjq4pRqSWVNHR9vyGf68DiCA/ztLsdnhQcH\n8NyV4wgO9Oeql1ZwsLTK7pJ8VqsHHY0xm0Tkb8AnQBmwBjh6lR4RuQuoBeY1Nb+IzAHmAMTGxpKR\nkdGuQktLS9s9r/JOK/JrKa2qpZ8ccHvb0PbXuutHCA8tr+Ci/3zB78aFEKh3i+p00taL7YjIA0Cu\nMeZJEbkS+BUwzRjT6rcM0tPTzcqVK9tVaEZGBlOnTm3XvMo7Xf/KKlbsLmTpH04iwN+9Hba0/Tnn\nvXV5/ObV1cxOTeDh80frNeldRERWGWPSW5vOqW4BIhJjjNkvIknAbGCiiEwH/g84wZkwV8qVDlfW\n8Pnm/Vw8PsntYa6cN2NUH3YWlPHIp1sZGBPODSem2F2ST3G2n9dbItILqAFuMMYUicgTQDDwqfUu\nvNQYc52b6lTqJz7esI/q2nrO1Gu3dDk3npTCzoJS/vHxFvpHh+nlGDqRU4FujJnSxDB961W2WbQ2\nj8Se3RibpF9/6GpEhIfOGUVOYQW3vbGGhMhujO6rr1Nn0M+qyuMcLK3iu+0HmKn3De2yQgL9efqy\nNKLDg7lm7kryiirsLsknaKArj/PB+r3U1Rv9MlEXFx0ezPNXjqOiuo6rX1pJeXVt6zOpDtFAVx7n\n3TV5DI4NZ2hchN2lqFYMju3OExensjm/hL+8t9HucryeBrryKHuKKliZVag3svAgU4fEcN0JA5m/\nPIePMvfaXY5X00BXHuXIfUNnjtZboHmSW08ezKjEHvz+rfXsLdbj6e6iga48yqI1eYzpG0lSr1C7\nS1FtEBTgx+MXplJTV89tr6+lrr5tX2hUztFAVx5j+/7DbNxboodbPFT/6DDunTmcJTsP8vTXO+wu\nxytpoCuPsWhNHn4CM0bpF1U81XlpiZwxKp5HPtnK2pwiu8vxOhroyiMYY1i0No9JA3sRExFidzmq\nnUSEB84eSWxECDe/tprSKu3K6Eoa6MojrMstZvfBcj3c4gV6hAby6AVjyD5Uzr2LNthdjlfRQFce\nYdHaPAL9henD9XCLNxjfP4rfnJjCm6tyj/ZcUh2nga66vLp6w3vr8jhhcAw9QgPtLke5yE3TBpGa\nFMmd76wnt1Av2OoKGuiqS6qqrWNL/mHeX7eXv76/iX0lVZylX/X3KgH+fjx+QSrGwK2vr6G2rt7u\nkjye3iZd2aq4vIbtBYfZsb+M7QWl7Nhfyo6CUrIPldOwq3Jav56cPCzWvkKVWyT1CuX+s0dwy+tr\neDJjBzdNG2R3SR5NA111ij1FFWzf7wjshsF9oLT66DRBAX4MiA5jeJ8ezBzdh4Ex4QzsHc6A3mGE\nBmlT9VZnpyaQsWU/j3++jeNSoknr19PukjyW/pcot6qurefOd9bz5qrco8N6dAskJSacaUNjGRgT\nRooV3Ik9Q/HX+1D6pPvOHsGq7EJufm01H9w8hYgQPVfSHhroym2KK2q47uVVLNl5kDnHD2Da0BhS\nYsKJCgvS65irn4gICeSxC1I5/+kl3LMwk8cuTLW7JI+kga7cIudQOVe9uILdB8t45PzRzB6baHdJ\nqotL69eTm6cN4pFPt3LCkN7MStU201bay0W53LrcImY9+T35JZW89MvxGubKaTecmML45CjuXriB\n7IPalbGtNNCVS322cR8XPL2U4AA/3r7+WI5Niba7JOVB/P2ERy8cgwjc/PpqarQrY5tooCuXmbtk\nN3NeXsmg2HDeueFYBsV2t7sk5YESIrvx4OyRrM4u4t+fb7O7HI+ix9BVh9XXGx74YBPPfruLk4fF\n8q+Lxmg3Q9UhM0b1IWNLAU98uZ3jUqKZMKCX3SV5BN1DVx1SWVPHr+f9wLPf7uLKY5N5+rI0DXPl\nEvfOHE5SVCi3vr6G4vIau8vxCBroqt0OlFZx0f+W8vHGfO6ecQz3zhyu/ciVy4QHB/D4hansP1zF\nnQvXY4ze5ag1GuiqXXYUlDL7ye/ZmFfCfy8Zy9WT+9tdkvJCo/tGctupg3l/3V4+3rDP7nK6PA10\n1WbLdx1i9pPfU1ZVy2tzJjJ9hF7SVrnPnCkDSIkJ528fbdZeL63QQFdtsmhtHpc+u4xe4UG88+vj\nSE3S624o9wrw9+MPpw1l14Ey5i/PtrucLk0DXTnFGMOTGdu5af5qxiRF8vb1x5LUK9TuspSPOGlo\nDBMHRPH4Z9s4XKknSJujga5aVVPnuMDW3z/awllj+vDy1eOJDA2yuyzlQ0SEu04/hoNl1Tz11Q67\ny+myNNBVq57+agfzl+fwmxNTePT8MQQH+NtdkvJBIxN7cNaYPjz7zS72FlfYXU6XpIGuWlRbV88r\nS7M5fnBvbv/FEPy0W6Ky0e2nDsEYePiTrXaX0iVpoKsWfb55P/kllVw6IcnuUpSib1QoVx6XzFs/\n5LIxr8TucrocDXTVoleWZhHfI4SThsbYXYpSANwwNYWIkEAe/HCT3aV0OU4FuojcLCKZIrJBRG6x\nhp1nPa4XkXT3lqnssPtAGd9sO8CF45II8Nf3ftU19AgN5MaTUvhm2wG+2lpgdzldSqv/pSIyArgW\nGA+MBmaISAqQCcwGvnZrhco285dn4+8nXDi+r92lKPUTl03qR9+objz4wSbq6vWSAEc4s9s1DFhm\njCk3xtQCXwGzjTGbjDFb3FuesktlTR1vrMzhlGGxxEaE2F2OUj8RHODP//1iKJvzD/PWD7mtz+Aj\nnAn0TGCKiPQSkVDgdEB32bzcR5n5FJbXcOnEfnaXolSTZoyKZ3TfSB7+ZAsV1XV2l9MltHqdU2PM\nJhH5G/AJUAasAZzeeiIyB5gDEBsbS0ZGRrsKLS0tbfe8qu3+s7SC2FChOnc9GXu0q6K2v67p9Pg6\nHsyp4q6XP2fmQP2ym1MXrjbGPAc8ByAiDwBOf8YxxjwDPAOQnp5upk6d2vYqgYyMDNo7r2qbzfkl\nbPvoG+46fRgnHT/A7nK6BG1/XdNUYGXpSj7ZcZA7L5hEdHiw3SXZytleLjHW7yQcJ0JfdWdRyl7z\nlmYTFODHuWl6c2fV9d1x2lAqaup4/DO9XZ2zfdHeEpGNwGLgBmNMkYjMEpFcYBLwvoh87LYqVacp\nq6rlndV7mDEqnp5h+hFWdX0De4dz0fi+vLo8mx0FpXaXYyunAt0YM8UYc4wxZrQx5nNr2DvGmERj\nTLAxJtYY8wv3lqo6w8I1eyitquWSCXoyVHmOm6cNJiTAj799uNnuUmyl3xZRRxljeGVpNsPiIxib\nFGl3OUo5rXf3YK47YSCfbNzH8l2H7C7HNhro6qjVOUVs2lvCJROSENGeLcqzXDNlALERwTzwwSaf\nvf+oBro6at7SbMKC/Dk7NcHuUpRqs25B/vz2lCGsySni/fV77S7HFhroCoCi8mreW5fHrLEJhAc7\n1ZtVqS7nnLREhsZ15+8fbaGq1ve+bKSBrgB4c1UuVbX1ejJUeTR/P+GO04aSfaicV5b63v1HNdAV\nxhjmLcsmrV9PhsVH2F2OUh1ywuDeTE6J5t9fbKO4wrfuP6qBrvh+x0F2HSjj0ol6Ewvl+USEP5w+\nlOKKGp78crvd5XQqDXTFK0uz6BkayGkj4u0uRSmXGN6nB7NSE3jh+93kFpbbXU6n0UD3cftKKvlk\n4z7OS+9LSKDe/Fl5j9tPHYIA//zYd67yrYHu415fkUNdveHi8Xq4RXmXPpHduGpyfxauyWN9brHd\n5XQKDXQfVltXz/zl2UwZFE1ydJjd5SjlctdPHUhUWJDPfNlIA92HfbmlgL3FldpVUXmtiJBAbjop\nhSU7D7Jkx0G7y3E7DXQf9srSLGIjgjl5WIzdpSjlNheOTyIqLIgXv99tdylup4Huo7IPlvP1tgIu\nHJdEgL82A+W9QgL9uXBcXz7btM/re7zof7KPenV5Nn4iXKQnQ5UPuMS6N+68Zd797VENdB9UVVvH\ngpU5TBsaQ1yPELvLUcrtEiK7ccoxsby2PJvKGu+9xosGug/6KDOfg2XVXDpRT4Yq33HFpGQKy2t4\nb533XolRA90HzVuaTb9eoUxOiba7FKU6zaSBvUiJCeel73d7bRdGDXQfs3XfYZbvPsTF45Pw89Ob\nWCjfISJcMakf6/cUszqnyO5y3EID3cfMW5pFUIAf56X3tbsUpTrdrLGJhAcHMNdLuzBqoPuQsqpa\n3v5hD2efozW6AAASM0lEQVSMjCcqLMjucpTqdOHBAZyblsj76/dScLjK7nJcTgPdhyxem8fhqlou\nmaBdFZXvumxSP2rqDK8t974ujBroPsIYwyvLshga1520fj3tLkcp2wzsHc6UQdHMW5ZNTV293eW4\nlAa6j1iXW0zmnhIumZCEiJ4MVb7tiknJ5JdU8unGfXaX4lIa6D7ilaVZhAb5c3Zqgt2lKGW7E4fG\nkNizGy952clRDXQfUFxew+J1eZydmkD3kEC7y1HKdv5+wmUT+7Fs1yE255fYXY7LaKD7gHnLs6is\nqdeToUo1cH56X4ID/Hjp+yy7S3EZDXQvl1dUwRNfbGfa0BiG9+lhdzlKdRk9w4I4a0wfFq7eQ3F5\njd3luIQGupe7b/FG6o3h3pnD7S5FqS7n8knJVNTUsWBVjt2luIQGuhf7cvN+PtqQz40nDaJvVKjd\n5SjV5YxI6EFav568vDSL+nrPv76LBrqXqqyp455FmQzsHca1UwbYXY5SXdYVxyaTdbCcr7YV2F1K\nh2mge6n/fLmdnEMV/OXsEQQF6MusVHOmD4+jd/dgr7i+i/6ne6EdBaU89dUOZqUmcOxAvUSuUi0J\nCvDj4vFJZGwtYPeBMrvL6RANdC9jjOHuhZl0C/TnztOH2V2OUh7h4glJ+IvwylLP7sLoVKCLyM0i\nkikiG0TkFmtYlIh8KiLbrN96gZAuYNHaPL7fcZDfTR9K7+7BdpejlEeIjQhh+og43liZQ3l1rd3l\ntFurgS4iI4BrgfHAaGCGiKQAdwCfG2MGAZ9bj5WNiitq+Mt7mxid2IOL9ebPSrXJFccmU1JZy8LV\neXaX0m7O7KEPA5YZY8qNMbXAV8Bs4CzgJWual4Cz3VOictbDn2zhUFkVf501En+9G5FSbZLeryfD\n4iOYu8Rzb1EX4MQ0mcBfRaQXUAGcDqwEYo0xR+62mg/ENjWziMwB5gDExsaSkZHRrkJLS0vbPa/d\nPs+u4ZPdNdyYGkJid/ectthVXMfLSyqZlhTAgW2rydjmlqfxWZ7c/pTzJkbV8MKGap555wuGRPnb\nXU6biTPvRCJyNfBroAzYAFQBVxpjIhtMU2iMafE4enp6ulm5cmW7Cs3IyGDq1KntmtdOVbV1TP7b\nlxQcriIyNJC5V41nVGJk6zO2QV294ez/fEd+SSWf//YEIvQCXC7nqe1PtU1FdR0TH/ycySnR/OeS\nsXaXc5SIrDLGpLc2nVO7i8aY54wxacaY44FCYCuwT0TirSeLB/Z3pGBv9f46x62u7j97BOHBAVz8\nv2Us33XIpc8xb1kW6/cUc/eMYzTMleqAbkH+XDCuLx9tyCe/uNLuctrM2V4uMdbvJBzHz18FFgFX\nWJNcAbzrjgI9mTGG577dxaCYcC6ZkMSC6yYRExHM5c8v46utrvlW2v7Dlfzjoy1MTonmzFHxLlmm\nUr7s0gn9qDeGV5d5XhdGZw/oviUiG4HFwA3GmCLgIeAUEdkGnGw9Vg2s2F3IhrwSfnlcf0SE+B7d\neONXk+gfHc41L63go8y9rS+kFX99fxNVtfXcd9ZwvRORUi6Q1CuUk4bE8OrybKpq6+wup02cPeQy\nxRhzjDFmtDHmc2vYQWPMNGPMIGPMycYY1x5H8ALPf7uLyNBAZjW4S1B0eDCvXTuREQk9uOHV1bz9\nQ267l//d9gO8uyaP604YwIDe4a4oWSkFXH5sMgdKq/koM9/uUtpEvynqJjmHyvlkYz4Xj0+iW9BP\nz5b3CA3klasnMKF/FLe9sZaX2/HttKraOu5+N5OkqFB+fWKKq8pWSgFTUqLpHx3Gix52fRcNdDeZ\nu2Q3fiJcNqlfk+PDggN4/spxnDwshrsXZvLUVzvatPz/fb2TnQVl3HfWcEICPa97lVJdmZ91i7rV\n2UWsyy2yuxynaaC7QWlVLa+tyOG0kfHE9+jW7HQhgf7899I0ZoyK56EPN/PPj7c49YWG7IPl/PuL\n7Zw+Mo6pQ2JcWbpSynJOWiKhQf7MXeI5J0c10N3grVW5HK6s5arjkludNtDfj8cvTOWC9L488eV2\n/rx4Y4sX2jfG8KdFmQT4CffM0LsQKeUuPbo5zn8tWpvHobJqu8txiga6i9XXG174bhepSZGkJjl3\nvTJ/P+Ghc0Zy1XH9efH73dzx9jrqmgn1jzfk8+WWAm49ZTBxPUJcWbpSqpHLJyVTXVvP6ys84xZ1\nGugu9uWW/ew+WM5Vx/Vv03wiwt0zhnHTtEG8sTKXm15bTXVt/U+mKauq5c+LNzIsPoIrj012YdVK\nqaYMievOxAFRvLI0i6yDZZRU1nTp67w4cy0X1QbPf7eL+B6OS3G2lYhw2ymDCQ/254EPNlNRXceT\nl4w9etLzsc+2sre4kicuHkuAv74XK9UZrjy2P9e9sooT/pEBQICf0DMsiF5hQfQMDSIqLIieYYFE\nHf3b+m09jgoL6rSOCxroLrQl/zDfbT/I76cPJbADgTvn+IGEBQfwx4WZ/PKFFfzvinRyDpXz/He7\nuWh8X9L66aXnleosvxgey6vXTGBvcSWF5dUcKqumsLyag6WO35vzSygsr6GwvJrmdt5Dg/x56tI0\njh/c2621aqC70Avf7SIk0I+Lxvft8LIumdCPsKAAfrtgLZc+uwwRx0ma308f6oJKlVLOEhGOTWn9\nVo519YbiipqjgX+o7MefwrJqEns23+PNVTTQXeRgaRVvr97DuWmJRIYGuWSZZ6cm0C3InxtfXU11\nXT3/OHeUy5atlHItfz85eojFLhroLjJ/eTbVtfX80sUnK38xPI65V49n2c5DnJuW6NJlK6W8iwa6\nC1TX1jN3SRbHD+7NoNjuLl/+xAG9mDigl8uXq5TyLtpVwgU+WL+X/YernPoikVJKuYtHBHppVS3b\ni7rmZSyNMTz/3S4G9g7j+EHuPYOtlFIt8YhAv3thJo+uquyS1yb+IbuQdbnFXHlcf/z0xsxKKRt5\nRKCfnZpAWQ18sanr3eXu+W93ExESwDljE1qfWCml3MgjAn1ySjQ9g4UFq9p/Mwh3yC0s58PMvVw0\nIYnQID2/rJSyl0cEur+fcFxCAF9tLWB/Sde5cevLS7IQES6flGx3KUop5RmBDjA5IYC6esM7q/fY\nXQoA5dW1zF+ezfQRcSREuv8bYEop1RqPCfS4MD/S+vXkzVW5XeJqZ2/9sIcSJ695rpRSncFjAh3g\n3LREtu0vZV1usa11HLnm+ejEHox18prnSinlbh4V6GeMiick0I8Fq+y92PxX2wrYWVDGVZP7I6Jd\nFZVSXYNHBXpESCDTh8exaE0elTX29Ul//ttdxEYEc9qIeNtqUEqpxjwq0AHOTetLSWUtn23aZ8vz\nb913mG+2HeDySckEBXjc5lNKeTGPS6RJA3vRp0cIC1ba0yf9he92Exzgx0Xjk2x5fqWUao7HBbq/\nn3BOWiLfbCsgv7hz+6QXllXzzupcZqUm2HrNY6WUaorHBTrAOWMTqTd0ep/0+Suyqayp55dtvAG0\nUkp1Bo8M9OToMMYl92TBqpxO65NeU1fP3O+zmJwSzZA411/zXCmlOsojAx3gvLS+7CwoY3VOUac8\n34eZ+eSXVHLV5OROeT6llGorjw3000fF0y3Qnzc76YJdz3+7i/7RYUwdHNMpz6eUUm3lsYEeHhzA\naSPiWLzW/X3Sf8guZE1OEVcem6zXPFdKdVkeG+gA56Yncriylo835LvtOYwxPPzJFsc1z/UmzUqp\nLsyjA31i/14kRHZz62GXhWv28N32g/xu+lDCg/Wa50qprsupQBeRW0Vkg4hkish8EQkRkdEiskRE\n1ovIYhGJcHexjflZfdK/3X6AvKIKly+/qLya+9/bxJi+kVyiXyRSSnVxrQa6iCQANwHpxpgRgD9w\nIfAscIcxZiTwDvA7dxbanHPHJmLc1Cf9oQ83U1RRw4OzR+qxc6VUl+fsIZcAoJuIBAChQB4wGPja\nGv8pcI7ry2tdUq9QJvSPcvl10pfvOsRrK3K4ZnJ/hsV3+ocPpZRqs1YPChtj9ojIP4FsoAL4xBjz\niYhsAM4CFgLnAX2bml9E5gBzAGJjY8nIyGhXoaWlpc3OOyKshud2VfPswi8Y1NO/XctvqLbecM/3\nFfQKEcYG5ZORYc+FwFTX0VL7U6qraDXQRaQnjuDuDxQBC0TkUuAq4F8icjewCKhuan5jzDPAMwDp\n6elm6tSp7So0IyOD5uYdV1XL/K2fsaO+N9dOHdWu5Tf0ny+3k1e6heeuSGfasNgOL095vpban1Jd\nhTOHXE4GdhljCowxNcDbwLHGmM3GmFONMWnAfGCHOwttSVhwAKePjOe9dXspr67t0LKyDpbxr8+3\ncdqIOA1zpZRHcSbQs4GJIhIqjtvzTAM2iUgMgIj4AX8EnnJfma07Ny2R0qqO9Uk3xvDHhZkE+vvx\npzOHu7A6pZRyv1YD3RizDHgT+AFYb83zDHCRiGwFNuM4SfqCG+ts1fjkKPpGdaxP+uJ1e/lm2wFu\nP3UwcT1CXFidUkq5n1PflDHG/An4U6PBj1s/XYKfn3Du2L489vlWcgvLSewZ2qb5iytquG/xRkYl\n9uCyScnuKVIppdzIo78p2tjssQkYA2//0PY+6X//aDOHyqp4YNZI/LXPuVLKA3lVoPeNCmXSgF5t\n7pO+KquQecuy+eVx/RmR0MONFSqllPt4VaADnJeeSPahcpbvOuTU9DV19dz1znr69AjhtlMGu7k6\npZRyH68L9Okj4ggLcv466c99u4vN+Ye5d+ZwwvTiW0opD+Z1gR4aFMAZo+J5f/1eyqpa7pOec6ic\nxz7byqnHxHLq8LhOqlAppdzD6wId4Lz0vpRX1/FhZvN90o0x3PNuJv4i3DtT+5wrpTyfVwZ6er+e\n9OsVypurcpqd5sPMfL7cUsBtpw6hT2S3TqxOKaXcwysDXUQ4d2wiS3ceIudQ+c/Gl1TWcO+iDQzv\nE8EVk/rZUKFSSrmeVwY6wOy0RERo8uTowx9v4UBpFQ/OHkmAv9duAqWUj/HaNEuI7MZxA6N564dc\n6ut/7JO+JqeIuUuzuHxSMqMSI22sUCmlXMtrAx0cF+zKLaxgmdUnvbaunjvfXk9M92B+e6r2OVdK\neRevDvRfDI+je3AAC6yToy9+v5uNe0u498zhdA8JtLk6pZRyLa8O9G5B/swYHc+H6/PZuu8wj3y6\nlWlDY5g+QvucK6W8j1cHOjgOu1TU1HHx/5ZiDPz5rOE4LuuulFLexesDfWxSTwZEh3GgtJpbTxnU\n5svqKqWUp/D6QBcRbpyWwi+Gx/LL4/rbXY5SSrmNT1yNalZqIrNSE+0uQyml3Mrr99CVUspXaKAr\npZSX0EBXSikvoYGulFJeQgNdKaW8hAa6Ukp5CQ10pZTyEhroSinlJcQY0/pUrnoykWJgWwuT9ACK\nmxkXDRxweVGdp6V185Tn7Mjy2jNvW+ZxZtrWpvHm9ged3wa1/bVtmpbG9zPG9G61CmNMp/0Az7R3\nPLCyM2vt7HX3hOfsyPLaM29b5nFmWl9uf+5oD539fL7c/pz96exDLos7ON6T2bFurn7OjiyvPfO2\nZR5npvXl9gedv37a/to2TYe3V6cecukIEVlpjEm3uw7lm7T9KU/gSSdFn7G7AOXTtP2pLs9j9tCV\nUkq1zJP20JVSSrVAA10ppbyEBrpSSnkJrwl0EQkTkZUiMsPuWpRvEZFhIvKUiLwpItfbXY/yXbYH\nuog8LyL7RSSz0fDpIrJFRLaLyB1OLOr3wBvuqVJ5K1e0P2PMJmPMdcD5wHHurFepltjey0VEjgdK\ngbnGmBHWMH9gK3AKkAusAC4C/IEHGy3iKmA00AsIAQ4YY97rnOqVp3NF+zPG7BeRmcD1wMvGmFc7\nq36lGrL9JtHGmK9FJLnR4PHAdmPMTgAReQ04yxjzIPCzQyoiMhUIA44BKkTkA2NMvTvrVt7BFe3P\nWs4iYJGIvA9ooCtb2B7ozUgAcho8zgUmNDexMeYuABG5Esceuoa56og2tT9rh2I2EAx84NbKlGpB\nVw30djHGvGh3Dcr3GGMygAyby1DK/pOizdgD9G3wONEaplRn0PanPFJXDfQVwCAR6S8iQcCFwCKb\na1K+Q9uf8ki2B7qIzAeWAENEJFdErjbG1AK/AT4GNgFvGGM22Fmn8k7a/pQ3sb3bolJKKdewfQ9d\nKaWUa2igK6WUl9BAV0opL6GBrpRSXkIDXSmlvIQGulJKeQkNdKWU8hIa6Eop5SU00JVSykv8P8xY\ni3EGkl0MAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14ac84f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (1-layer net)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # 1 Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "    \n",
    "  # 2 Variables.\n",
    "  weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "\n",
    "  weights2 = tf.Variable(tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    \n",
    "  # 3 Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels))\n",
    "  \n",
    "\n",
    "  # 4 Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "    \n",
    "  # 5 Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-20607ad686f5>:5: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 307.827240\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 27.0%\n",
      "Minibatch loss at step 2: 1321.388184\n",
      "Minibatch accuracy: 46.1%\n",
      "Validation accuracy: 41.3%\n",
      "Minibatch loss at step 4: 120.429474\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 56.7%\n",
      "Minibatch loss at step 6: 9.355587\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 62.6%\n",
      "Minibatch loss at step 8: 7.533780\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 10: 4.323696\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 62.5%\n",
      "Minibatch loss at step 12: 3.719761\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 62.5%\n",
      "Minibatch loss at step 14: 3.167106\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 16: 2.658126\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 18: 2.146497\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 20: 1.657254\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 62.5%\n",
      "Minibatch loss at step 22: 1.182319\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 62.5%\n",
      "Minibatch loss at step 24: 0.706164\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 62.5%\n",
      "Minibatch loss at step 26: 0.289699\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 61.6%\n",
      "Minibatch loss at step 28: 0.353315\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 61.6%\n",
      "Minibatch loss at step 30: 0.372657\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 61.6%\n",
      "Minibatch loss at step 32: 0.368801\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 61.6%\n",
      "Minibatch loss at step 34: 0.364939\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 61.6%\n",
      "Minibatch loss at step 36: 0.361153\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 61.6%\n",
      "Minibatch loss at step 38: 0.357234\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 61.6%\n",
      "Minibatch loss at step 40: 0.353350\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 61.6%\n",
      "Minibatch loss at step 42: 0.349291\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 61.6%\n",
      "Minibatch loss at step 44: 0.345131\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 61.6%\n",
      "Minibatch loss at step 46: 0.340853\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 61.6%\n",
      "Minibatch loss at step 48: 0.336435\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 61.6%\n",
      "Minibatch loss at step 50: 0.331864\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 61.6%\n",
      "Minibatch loss at step 52: 0.327611\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 61.6%\n",
      "Minibatch loss at step 54: 0.322685\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 61.6%\n",
      "Minibatch loss at step 56: 0.317539\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 61.6%\n",
      "Minibatch loss at step 58: 0.312155\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 61.6%\n",
      "Minibatch loss at step 60: 0.307987\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 61.6%\n",
      "Minibatch loss at step 62: 0.289250\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 61.6%\n",
      "Minibatch loss at step 64: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.5%\n",
      "Minibatch loss at step 66: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.5%\n",
      "Minibatch loss at step 68: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.5%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.5%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.5%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.5%\n",
      "Minibatch loss at step 76: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.5%\n",
      "Minibatch loss at step 78: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.5%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.5%\n",
      "Minibatch loss at step 82: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.5%\n",
      "Minibatch loss at step 84: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.5%\n",
      "Minibatch loss at step 86: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.5%\n",
      "Minibatch loss at step 88: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.5%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.5%\n",
      "Minibatch loss at step 92: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.5%\n",
      "Minibatch loss at step 94: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.5%\n",
      "Minibatch loss at step 96: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.5%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.5%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.5%\n",
      "Test accuracy: 68.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101 # decrease this number\n",
    "num_bacthes = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "    \n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_bacthes\n",
    "    \n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are far too much parameters and no regularization, the accuracy of the batches is 100%. **The generalization capability is poor**, as shown in the validation and test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # 1 Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "\n",
    "  # 2 Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "\n",
    "  # 3 Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5) # add dropout to our training\n",
    "  logits = tf.matmul(drop1, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels))\n",
    "    \n",
    "    \n",
    "  # 4 Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "\n",
    "  # 5 Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-21-feaa633773ac>:5: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 448.274292\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 24.4%\n",
      "Minibatch loss at step 2: 1192.446777\n",
      "Minibatch accuracy: 49.2%\n",
      "Validation accuracy: 31.9%\n",
      "Minibatch loss at step 4: 238.649506\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 47.6%\n",
      "Minibatch loss at step 6: 170.111343\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 61.7%\n",
      "Minibatch loss at step 8: 9.548517\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 10: 8.609098\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 12: 8.126238\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 67.6%\n",
      "Minibatch loss at step 14: 2.995707\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 67.3%\n",
      "Minibatch loss at step 16: 5.602012\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 65.0%\n",
      "Minibatch loss at step 18: 5.971211\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 64.7%\n",
      "Minibatch loss at step 20: 2.408777\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.8%\n",
      "Minibatch loss at step 22: 1.387986\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 66.5%\n",
      "Minibatch loss at step 24: 7.067874\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.0%\n",
      "Minibatch loss at step 26: 4.430592\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 67.6%\n",
      "Minibatch loss at step 28: 10.602009\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.3%\n",
      "Minibatch loss at step 30: 9.753213\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.5%\n",
      "Minibatch loss at step 32: 1.800819\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 67.6%\n",
      "Minibatch loss at step 34: 3.163637\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 36: 0.883358\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 65.8%\n",
      "Minibatch loss at step 38: 5.429131\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 40: 3.639129\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 65.6%\n",
      "Minibatch loss at step 42: 0.707295\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.8%\n",
      "Minibatch loss at step 44: 0.135789\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 46: 0.870229\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 48: 12.033650\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 50: 13.781413\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 52: 9.421358\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 65.0%\n",
      "Minibatch loss at step 54: 6.878296\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 64.6%\n",
      "Minibatch loss at step 56: 0.236725\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 65.1%\n",
      "Minibatch loss at step 58: 20.065901\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 65.8%\n",
      "Minibatch loss at step 60: 2.539353\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 64.9%\n",
      "Minibatch loss at step 62: 8.239712\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 65.8%\n",
      "Minibatch loss at step 64: 0.467696\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 67.2%\n",
      "Minibatch loss at step 66: 2.678029\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 68: 0.882241\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.3%\n",
      "Minibatch loss at step 70: 0.362076\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.5%\n",
      "Minibatch loss at step 72: 4.844646\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 74: 9.813749\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.4%\n",
      "Minibatch loss at step 76: 7.084088\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 65.0%\n",
      "Minibatch loss at step 78: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 80: 12.174295\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 66.1%\n",
      "Minibatch loss at step 82: 5.589087\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 65.1%\n",
      "Minibatch loss at step 84: 0.224920\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.5%\n",
      "Minibatch loss at step 86: 10.591427\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 64.9%\n",
      "Minibatch loss at step 88: 0.917638\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 90: 1.446891\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 64.1%\n",
      "Minibatch loss at step 92: 10.008069\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 63.1%\n",
      "Minibatch loss at step 94: 1.432094\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 96: 5.554665\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 65.2%\n",
      "Minibatch loss at step 98: 3.177362\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 63.8%\n",
      "Minibatch loss at step 100: 2.331326\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.3%\n",
      "Test accuracy: 73.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    \n",
    "    offset = step % num_batches\n",
    "    \n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "    \n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first conclusion is that 100% of accuracy on the minibatches is more difficult achieved or to keep. As a result, the test accuracy is improved by 6%, the final net is more capable of generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try 2-layer network. Note how the parameters are initialized, compared to the previous cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 100\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # 1 Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0) # count the number of steps taken\n",
    "\n",
    "\n",
    "  # 2 Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "    \n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "    \n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "\n",
    "  # 3 Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  logits = tf.matmul(lay2_train, weights3) + biases3\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "  \n",
    "\n",
    "  # 4 Optimizer.\n",
    "    # learning rate\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase = True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)\n",
    "  \n",
    "\n",
    "  # 5 Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-23-a5e70c61c44d>:4: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 3.368782\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 26.0%\n",
      "Minibatch loss at step 500: 1.009399\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 1000: 0.868829\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 1500: 0.643913\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 2000: 0.795483\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 2500: 0.405569\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 3000: 0.619974\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 3500: 0.543180\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 4000: 0.618144\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 4500: 0.407751\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 5000: 0.377069\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 5500: 0.439695\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 6000: 0.326182\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 6500: 0.484140\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 7000: 0.427936\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 7500: 0.416866\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 8000: 0.396937\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 8500: 0.386866\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 9000: 0.379783\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.3%\n",
      "Test accuracy: 95.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "    \n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    \n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "    \n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is getting really good 95.6%. Let's try one layer deeper(3-layer network) with dropouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256 # we can try different parameters here\n",
    "num_hidden_nodes3 = 128\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # 1 Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "\n",
    "  # 2 Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "    \n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "    \n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "    \n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "\n",
    "  # 3 Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
    "  logits = tf.matmul(lay3_train, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels))\n",
    "  \n",
    "    \n",
    "  # 4 Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase = True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)\n",
    "  \n",
    "    \n",
    "  # 5 Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-25-6c8d4513639a>:4: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 2.427080\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 34.5%\n",
      "Minibatch loss at step 500: 0.432839\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 1000: 0.404058\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 1500: 0.369126\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 2000: 0.485151\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 2500: 0.193783\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 3000: 0.351544\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 3500: 0.295934\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 4000: 0.414581\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 4500: 0.212324\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 5000: 0.184641\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 5500: 0.220025\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 6000: 0.107669\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 6500: 0.191686\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 7000: 0.177449\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 7500: 0.130062\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 8000: 0.162816\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 8500: 0.111788\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 9000: 0.138030\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 9500: 0.156099\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 10000: 0.195372\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 10500: 0.149206\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 11000: 0.203603\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 11500: 0.086927\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 12000: 0.070652\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 12500: 0.140262\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 13000: 0.052105\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 13500: 0.056783\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 14000: 0.097896\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 14500: 0.033319\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 15000: 0.062255\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 15500: 0.035821\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 16000: 0.091924\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 16500: 0.036352\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 17000: 0.079543\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 17500: 0.024671\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 18000: 0.028776\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 90.8%\n",
      "Test accuracy: 96.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 18001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "    \n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    \n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "    \n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a pretty good result. We can also try more parameters as I comment in the cell."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
